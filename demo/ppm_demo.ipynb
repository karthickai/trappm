{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Predictive Model Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate ONNX model using timm library\n",
    "### You can generate your custom model from PyTorch or TensorFlow into ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timm.models import create_model\n",
    "import torch\n",
    "from timm.models import create_model\n",
    "from timm.data import resolve_data_config\n",
    "\n",
    "\n",
    "def get_model(model_name, model_dtype, device, drop_rate):\n",
    "    model = create_model(\n",
    "        model_name, pretrained=False, in_chans=3, exportable=True, drop_rate=drop_rate, scriptable=False)\n",
    "    model.to(device=device, dtype=model_dtype)\n",
    "    data_config = resolve_data_config({}, model=model)\n",
    "    input_size = data_config['input_size']\n",
    "    num_classes = model.num_classes\n",
    "    return model, input_size, num_classes\n",
    "\n",
    "def get_input(batch_size, input_size, data_dtype, device):\n",
    "    dummy_inputs = torch.randn(\n",
    "        (batch_size,) + input_size, device=device, dtype=data_dtype)\n",
    "    return dummy_inputs\n",
    "\n",
    "def to_onnx(model, dummy_inputs, onnx_file):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        torch.onnx.export(model, dummy_inputs, onnx_file, verbose=True, export_params=False)\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Give model name and batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"vgg16\"\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "model_dtype = torch.float32\n",
    "model, input_size, num_classes = get_model(MODEL_NAME, model_dtype, device, 0.0)\n",
    "dummy_inputs = get_input(BATCH_SIZE, input_size, model_dtype, device)\n",
    "onnx_file = f'{MODEL_NAME}_{BATCH_SIZE}.onnx'\n",
    "to_onnx(model, dummy_inputs, onnx_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TraPPM prediction on single A100 GPU 40GB HBM\n",
    "#### code: github.com/karthickai/trappm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import trappm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trappm.predict(\"vgg16_128.onnx\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
